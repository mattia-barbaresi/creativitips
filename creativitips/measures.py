"""Module for models tests and  measures"""
# from Learning a Generative Probabilistic Grammar of Experience: A Process-Level Model of Language Acquisition
# (https://onlinelibrary.wiley.com/doi/pdfdirect/10.1111/cogs.12140)
#  generative model can be evaluated with regard to its precision and
# recallâ€”two customary measures of performance in natural language engineering, which
# can address the perennial questions of model relevance and scalability.

# tests:
# both general (precision and recall)
# and specific (ranging from word segmentation to structure-dependent syntactic generalization).
# Characteristics of the learned representation: equivalence (substitutability)
# of phrases and the similarity structure of the phrase space
import json
import math
import os
import matplotlib.pyplot as plt
import numpy as np


def recall(tp, fn):
    """defined as in https://livrepository.liverpool.ac.uk/3028272/1/McCauley_Christiansen_in_press_Psych_Rev.pdf"""
    # """ defined as the proportion of sentences in a corpus withheld for testing that the model can generate
    #  (see Solan et al., 2005; for an earlier use and for a discussion of their roots in information retrieval)"""
    if tp + fn > 0:
        return tp / (tp + fn)
    else:
        return 0


def precision(tp, fp):
    """defined as in https://livrepository.liverpool.ac.uk/3028272/1/McCauley_Christiansen_in_press_Psych_Rev.pdf"""
    # """defined as the proportion of sentences generated by it that are found acceptable by human judges"""
    # To estimate the precision of the grammar learned by U-MILA and compare it to a trigram model,
    # they conducted two experiments in which participants were asked to rate the
    # acceptability of 50 sentences generated by each of the two models, which had been mixed with 50 sentences
    # from the original corpus (150 sentences altogether, ordered randomly).
    # Sentences were scored for their acceptability on a scale of 1 (not acceptable) to 7 (completely acceptable; Waterfall et al., 2010).
    # As the 50 sentences chosen from the original corpus ranged in length between three and eleven words,
    # in the analysis we excluded shorter and longer sentences generated...
    if tp + fp > 0:
        return tp / (tp + fp)
    else:
        return 0


def f_score(prec, rec, beta=1.0):
    if prec + rec != 0:
        return (1 + beta**2) * (prec * rec) / ((beta**2 * prec) + rec)
    else:
        return 0


def perplexity(model, sequences, n_words):
    """graded counterpart for recall:
     the (negative logarithm of the) mean probability assigned by the model to sentences from the test corpus
    (see, e.g., Goodman, 2001; for a definition)"""
    # They used a trained version of the model to calculate the production probability of each of the 100 utterances
    # in the test set, and the perplexity over it, using a standard formula (Jelinek, 1990; Stolcke, 2010):
    # where P(s) is the probability of a sentence s, the sum is over all the sentences in the test set,
    # and n is the number of words in the test set.
    tot = 0
    for s in sequences:
        tot += math.log(model.get_prob(s))
    return 10 ** - (tot / n_words)


def calculate_stats(data, ref_key=""):
    if not ref_key:
        ref_key = data.keys()[0]
    res_stats = dict()
    res_stats["par"] = dict()
    res_stats["par"]["word_pairs"] = 0
    res_stats["par"]["bound_n"] = 0
    mods_keys = list([k for k in data.keys() if k != ref_key])
    for k in mods_keys:
        res_stats[k] = {"tp": 0, "fn": 0, "fp": 0}

    # for each line
    for idx, line_ref in enumerate(data[ref_key]):
        ref_arr = line_ref
        res_stats["par"]["bound_n"] += len([x for x in line_ref if x == "||"])
        res_stats["par"]["word_pairs"] += (len([x for x in line_ref if x != "||"]) - 1)
        # count occurences for beta
        # for each model
        for curr_key in mods_keys:
            curr_arr = data[curr_key][idx]
            # calculate stats
            ctp = 0
            cfn = 0
            cfp = 0
            j = 0
            i = 0
            # print("ref_arr: ", ref_arr)
            # print("curr_arr, {} : ".format(curr_key), curr_arr)
            while i < len(ref_arr):
                # no neutral cases!
                # true positive
                if ref_arr[i] == "||" and curr_arr[j] == "||":
                    ctp += 1
                    i += 1
                    j += 1
                # false positive
                elif ref_arr[i] != "||" and curr_arr[j] == "||":
                    cfp += 1
                    j += 1
                # false negative
                elif ref_arr[i] == "||" and curr_arr[j] != "||":
                    cfn += 1
                    i += 1
                # true negative, not collected
                elif ref_arr[i] == curr_arr[j]:
                    i += 1
                    j += 1
                else:
                    if ref_arr[i] != curr_arr[j]:
                        print("---------------------------------------")
                        print("diff: ", ref_arr[i], curr_arr[j])
                        print("{}   : ".format(ref_key), ref_arr)
                        print("{}   : ".format(curr_key), curr_arr)
                    else:
                        print("WHAAAAAAAAAAAAAAAAAAAAAAAAAAAAT !!!!!!!!")
                    j += 1  # skip word in curr cause ref (illinois localPipeline) has a problem

            res_stats[curr_key]["tp"] += ctp
            res_stats[curr_key]["fn"] += cfn
            res_stats[curr_key]["fp"] += cfp

    res_stats["par"]["beta"] = res_stats["par"]["bound_n"] / res_stats["par"]["word_pairs"]
    for ck in mods_keys:
        res_stats[ck]["recall"] = recall(res_stats[ck]["tp"], res_stats[ck]["fn"])
        res_stats[ck]["precision"] = precision(res_stats[ck]["tp"], res_stats[ck]["fp"])
        res_stats[ck]["f_score"] = f_score(res_stats[ck]["precision"],  res_stats[ck]["recall"], beta=res_stats["par"]["beta"])

    return res_stats


def draw_boxplot(res, dir_out):
    data = [[],[]]
    fig, ax = plt.subplots()
    # Creating axes instance
    ax.set_title('F-score for TiPs and CBL')

    for rf, vf in res.items():
        for mk, mv in vf.items():
            # Creating dataset
            data[0].append(res[rf]["cbl"]["f_score"])
            data[1].append(res[rf]["tips"]["f_score"])

    # Creating plot
    bp = ax.boxplot(data, labels=["cbl", "tips"])
    plt.yticks(np.arange(0, 1.1, step=0.1))
    # show plot
    plt.savefig(dir_out + 'boxplot.pdf')
    plt.show()


def collect_data(ref_dir, root_out, model_dirs):
    results = {}
    data = {}

    for subdir, dirs, files in os.walk(ref_dir):
        if "Edinburgh" in subdir:
            continue
        for fn in files:
            if '.capp' in fn:
                data[fn] = {}
                for mk, mv in model_dirs.items():
                    m_dir = subdir.replace(ref_dir, mv)
                    file_ext = '.shpar' + (mk if mk != "isp" else "")
                    data[fn][mk] = []
                    with open(m_dir + '/' + fn.split('.capp')[0] + file_ext, 'r', encoding="utf-8") as fpi:
                        print("processing file:", fpi)
                        for line in fpi.readlines():
                            arr = line.strip().split()
                            data[fn][mk].append(arr)

            results[fn] = calculate_stats(data[fn], "isp")

    tots = {
        "cbl": {"tot_tp": 0, "tot_fn": 0, "tot_fp": 0},
        "tips": {"tot_tp": 0, "tot_fn": 0, "tot_fp": 0},
        "par": {"bound_n": 0, "word_pairs": 0, "beta_sum": 0}
    }

    for rf, vf in results.items():
        for mk, mv in vf.items():
            if mk != "par":
                tots[mk]["tot_tp"] += mv["tp"]
                tots[mk]["tot_fp"] += mv["fp"]
                tots[mk]["tot_fn"] += mv["fn"]
            else:
                tots["par"]["bound_n"] += vf["par"]["bound_n"]
                tots["par"]["word_pairs"] += vf["par"]["word_pairs"]
                tots["par"]["beta_sum"] += vf["par"]["beta"]

    tots["par"]["beta_avg"] = float(tots["par"]["beta_sum"]) / len(results.keys())
    tots["par"]["beta"] = tots["par"]["bound_n"] / tots["par"]["word_pairs"]

    for k in [x for x in tots.keys() if x != "par"]:
        tots[k]["precision"] = precision(tots[k]["tot_tp"], tots[k]["tot_fp"])
        tots[k]["recall"] = recall(tots[k]["tot_tp"], tots[k]["tot_fn"])
        tots[k]["f_score"] = f_score(tots[k]["precision"], tots[k]["recall"], beta=tots["par"]["beta"])

    draw_boxplot(results, root_out)
    with open(root_out + "results.json", "w") as fpo:
        json.dump(results, fpo)
    with open(root_out + "aggregated.json", "w") as fpo:
        json.dump(tots, fpo)


def collect_arrays(dir_in):
    data = json.load(open(dir_in + "results.json","r"))
    with open(dir_in + "cbl_arr.txt", "w") as fpc:
        with open(dir_in + "tips_arr.txt", "w") as fpt:
            for itm, vl in data.items():
                fpc.write(str(vl["cbl"]["f_score"]) + "\n")
                fpt.write(str(vl["tips"]["f_score"]) + "\n")


def convergence_data(rootd):
    for pars_mem in [50,500,1000]:
        rd = "tps_results_pars_{}/".format(pars_mem)
        tps_data = {}
        for rip in [10,100,500,1000,5000,10000]:
            ripd = "tps_results_{}/".format(rip)
            for met in ["BRENT", "AVG", "FTPAVG"]:
                metd = "{}_NFWI/thompson_newport_train/".format(met)
                with open(rd + ripd + metd + "test_gen.json","r") as fpi:
                    data = json.load(fpi)
                    print(data)

    fig, ax = plt.subplots()
    # Creating axes instance
    ax.set_title('divergence: TPS vs. generalized')

    for rf, vf in res.items():
        for mk, mv in vf.items():
            # Creating dataset
            data[0].append(res[rf]["cbl"]["f_score"])
            data[1].append(res[rf]["tips"]["f_score"])

    # Creating plot
    bp = ax.boxplot(data, labels=["cbl", "tips"])
    plt.yticks(np.arange(0, 1.1, step=0.1))
    # show plot
    plt.savefig(dir_out + 'boxplot.pdf')
    plt.show()



if __name__ == "__main__":
    rd = "data/CHILDES_converted/"
    ro = "data/CHILDES_results2_BRENT_NFWI/"
    # os.makedirs(ro, exist_ok=True)
    # mdls = {
    #     "isp": "data/CHILDES_ISP/",
    #     "cbl": "data/CHILDES_cbl/",
    #     "tips": "data/CHILDES_tips2_BRENT_NFWI/"
    # }
    # collect_data(rd, ro, mdls)
    # collect_arrays(ro)
    collect_convergence_data("/data/out")
