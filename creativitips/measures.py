"""Module for models tests and  measures"""
# from Learning a Generative Probabilistic Grammar of Experience: A Process-Level Model of Language Acquisition
# (https://onlinelibrary.wiley.com/doi/pdfdirect/10.1111/cogs.12140)
#  generative model can be evaluated with regard to its precision and
# recallâ€”two customary measures of performance in natural language engineering, which
# can address the perennial questions of model relevance and scalability.

# tests:
# both general (precision and recall)
# and specific (ranging from word segmentation to structure-dependent syntactic generalization).
# Characteristics of the learned representation: equivalence (substitutability)
# of phrases and the similarity structure of the phrase space
import json
import math
import os
import statistics
from difflib import SequenceMatcher
import creativity as ct
import matplotlib.pyplot as plt
import networkx as nx
import numpy as np
import pandas as pd
from thefuzz import fuzz

parser_decays = [20, 50, 100, 500, 1000]


def recall(tp, fn):
    """defined as in https://livrepository.liverpool.ac.uk/3028272/1/McCauley_Christiansen_in_press_Psych_Rev.pdf"""
    # """ defined as the proportion of sentences in a corpus withheld for testing that the model can generate
    #  (see Solan et al., 2005; for an earlier use and for a discussion of their roots in information retrieval)"""
    if tp + fn > 0:
        return tp / (tp + fn)
    else:
        return 0


def precision(tp, fp):
    """defined as in https://livrepository.liverpool.ac.uk/3028272/1/McCauley_Christiansen_in_press_Psych_Rev.pdf"""
    # """defined as the proportion of sentences generated by it that are found acceptable by human judges"""
    # To estimate the precision of the grammar learned by U-MILA and compare it to a trigram model,
    # they conducted two experiments in which participants were asked to rate the
    # acceptability of 50 sentences generated by each of the two models, which had been mixed with 50 sentences
    # from the original corpus (150 sentences altogether, ordered randomly).
    # Sentences were scored for their acceptability on a scale of 1 (not acceptable) to 7 (completely acceptable; Waterfall et al., 2010).
    # As the 50 sentences chosen from the original corpus ranged in length between three and eleven words,
    # in the analysis we excluded shorter and longer sentences generated...
    if tp + fp > 0:
        return tp / (tp + fp)
    else:
        return 0


def f_score(prec, rec, beta=1.0):
    if prec + rec != 0:
        return (1 + beta ** 2) * (prec * rec) / ((beta ** 2 * prec) + rec)
    else:
        return 0


def perplexity(model, sequences, n_words):
    """graded counterpart for recall:
     the (negative logarithm of the) mean probability assigned by the model to sentences from the test corpus
    (see, e.g., Goodman, 2001; for a definition)"""
    # They used a trained version of the model to calculate the production probability of each of the 100 utterances
    # in the test set, and the perplexity over it, using a standard formula (Jelinek, 1990; Stolcke, 2010):
    # where P(s) is the probability of a sentence s, the sum is over all the sentences in the test set,
    # and n is the number of words in the test set.
    tot = 0
    for s in sequences:
        tot += math.log(model.get_prob(s))
    return 10 ** - (tot / n_words)


def calculate_stats(data, ref_key=""):
    if not ref_key:
        ref_key = data.keys()[0]
    res_stats = dict()
    res_stats["par"] = dict()
    res_stats["par"]["word_pairs"] = 0
    res_stats["par"]["bound_n"] = 0
    mods_keys = list([k for k in data.keys() if k != ref_key])
    for k in mods_keys:
        res_stats[k] = {"tp": 0, "fn": 0, "fp": 0}

    # for each line
    for idx, line_ref in enumerate(data[ref_key]):
        ref_arr = line_ref
        res_stats["par"]["bound_n"] += len([x for x in line_ref if x == "||"])
        res_stats["par"]["word_pairs"] += (len([x for x in line_ref if x != "||"]) - 1)
        # count occurences for beta
        # for each model
        for curr_key in mods_keys:
            curr_arr = data[curr_key][idx]
            # calculate stats
            ctp = 0
            cfn = 0
            cfp = 0
            j = 0
            i = 0
            # print("ref_arr: ", ref_arr)
            # print("curr_arr, {} : ".format(curr_key), curr_arr)
            while i < len(ref_arr):
                # no neutral cases!
                # true positive
                if ref_arr[i] == "||" and curr_arr[j] == "||":
                    ctp += 1
                    i += 1
                    j += 1
                # false positive
                elif ref_arr[i] != "||" and curr_arr[j] == "||":
                    cfp += 1
                    j += 1
                # false negative
                elif ref_arr[i] == "||" and curr_arr[j] != "||":
                    cfn += 1
                    i += 1
                # true negative, not collected
                elif ref_arr[i] == curr_arr[j]:
                    i += 1
                    j += 1
                else:
                    if ref_arr[i] != curr_arr[j]:
                        print("---------------------------------------")
                        print("diff: ", ref_arr[i], curr_arr[j])
                        print("{}   : ".format(ref_key), ref_arr)
                        print("{}   : ".format(curr_key), curr_arr)
                    else:
                        print("WHAAAAAAAAAAAAAAAAAAAAAAAAAAAAT !!!!!!!!")
                    j += 1  # skip word in curr cause ref (illinois localPipeline) has a problem

            res_stats[curr_key]["tp"] += ctp
            res_stats[curr_key]["fn"] += cfn
            res_stats[curr_key]["fp"] += cfp

    res_stats["par"]["beta"] = res_stats["par"]["bound_n"] / res_stats["par"]["word_pairs"]
    for ck in mods_keys:
        res_stats[ck]["recall"] = recall(res_stats[ck]["tp"], res_stats[ck]["fn"])
        res_stats[ck]["precision"] = precision(res_stats[ck]["tp"], res_stats[ck]["fp"])
        res_stats[ck]["f_score"] = f_score(res_stats[ck]["precision"], res_stats[ck]["recall"],
                                           beta=res_stats["par"]["beta"])

    return res_stats


def draw_boxplot(res, dir_out):
    data = [[], []]
    fig, ax = plt.subplots()
    # Creating axes instance
    ax.set_title('F-score for TiPs and CBL')

    for rf, vf in res.items():
        for mk, mv in vf.items():
            # Creating dataset
            data[0].append(res[rf]["cbl"]["f_score"])
            data[1].append(res[rf]["tips"]["f_score"])

    # Creating plot
    bp = ax.boxplot(data, labels=["cbl", "tips"])
    plt.yticks(np.arange(0, 1.1, step=0.1))
    # show plot
    plt.savefig(dir_out + 'boxplot.pdf')
    plt.show()


def collect_data(ref_dir, root_out, model_dirs):
    results = {}
    data = {}

    for subdir, dirs, files in os.walk(ref_dir):
        if "Edinburgh" in subdir:
            continue
        for fn in files:
            if '.capp' in fn:
                data[fn] = {}
                for mk, mv in model_dirs.items():
                    m_dir = subdir.replace(ref_dir, mv)
                    file_ext = '.shpar' + (mk if mk != "isp" else "")
                    data[fn][mk] = []
                    with open(m_dir + '/' + fn.split('.capp')[0] + file_ext, 'r', encoding="utf-8") as fpi:
                        print("processing file:", fpi)
                        for line in fpi.readlines():
                            arr = line.strip().split()
                            data[fn][mk].append(arr)

            results[fn] = calculate_stats(data[fn], "isp")

    tots = {
        "cbl": {"tot_tp": 0, "tot_fn": 0, "tot_fp": 0},
        "tips": {"tot_tp": 0, "tot_fn": 0, "tot_fp": 0},
        "par": {"bound_n": 0, "word_pairs": 0, "beta_sum": 0}
    }

    for rf, vf in results.items():
        for mk, mv in vf.items():
            if mk != "par":
                tots[mk]["tot_tp"] += mv["tp"]
                tots[mk]["tot_fp"] += mv["fp"]
                tots[mk]["tot_fn"] += mv["fn"]
            else:
                tots["par"]["bound_n"] += vf["par"]["bound_n"]
                tots["par"]["word_pairs"] += vf["par"]["word_pairs"]
                tots["par"]["beta_sum"] += vf["par"]["beta"]

    tots["par"]["beta_avg"] = float(tots["par"]["beta_sum"]) / len(results.keys())
    tots["par"]["beta"] = tots["par"]["bound_n"] / tots["par"]["word_pairs"]

    for k in [x for x in tots.keys() if x != "par"]:
        tots[k]["precision"] = precision(tots[k]["tot_tp"], tots[k]["tot_fp"])
        tots[k]["recall"] = recall(tots[k]["tot_tp"], tots[k]["tot_fn"])
        tots[k]["f_score"] = f_score(tots[k]["precision"], tots[k]["recall"], beta=tots["par"]["beta"])

    draw_boxplot(results, root_out)
    with open(root_out + "results.json", "w") as fpo:
        json.dump(results, fpo)
    with open(root_out + "aggregated.json", "w") as fpo:
        json.dump(tots, fpo)


def collect_arrays(dir_in):
    data = json.load(open(dir_in + "results.json", "r"))
    with open(dir_in + "cbl_arr.txt", "w") as fpc:
        with open(dir_in + "tips_arr.txt", "w") as fpt:
            for itm, vl in data.items():
                fpc.write(str(vl["cbl"]["f_score"]) + "\n")
                fpt.write(str(vl["tips"]["f_score"]) + "\n")


def divergence_convergence_thompson_newport(rootd):
    tps_data = {}
    for pars_mem in parser_decays:
        tps_data[pars_mem] = {}
        rd = "tps_results_pars_{}/".format(pars_mem)
        for rip in [10, 100, 500, 1000, 5000, 10000]:
            tps_data[pars_mem][rip] = {}
            ripd = "tps_results_{}/".format(rip)
            for met in ["BRENT_NFWI", "AVG_NFWI", "FTPAVG_NFWI"]:
                tps_data[pars_mem][rip][met] = {}
                dirin = rootd + rd + ripd + "{}/".format(met)

                # nebrelsot
                with open(dirin + "thompson_newport_nebrelsot/nebrelsot.json", "r") as fpi:
                    tps_data[pars_mem][rip][met]["nebrelsot"] = json.load(fpi)
                with open(dirin + "thompson_newport_ABCDEF_nebrelsot/nebrelsot.json", "r") as fpi:
                    tps_data[pars_mem][rip][met]["ABCDEF_nebrelsot"] = json.load(fpi)

                # train-test
                with open(dirin + "thompson_newport_train/test_gen.json", "r") as fpi:
                    tps_data[pars_mem][rip][met]["train"] = json.load(fpi)

                # convergence
                tps_data[pars_mem][rip][met]["but_last"] = {"u2": {}, "u3": {}}
                with open(dirin + "thompson_newport_but_last/gens_dataM2.json", "r") as fpi:
                    tps_data[pars_mem][rip][met]["but_last"]["u2"] = json.load(fpi)["results"]
                with open(dirin + "thompson_newport_but_last/gens_dataM3.json", "r") as fpi:
                    tps_data[pars_mem][rip][met]["but_last"]["u3"] = json.load(fpi)["results"]
    with open(rootd + "thompson_newport_results.json", "w") as fpo:
        json.dump(tps_data, fpo)


def tables_data_thompson_newport_nebrelsot(rootd):
    whole_d = {}
    for fname in ["nebrelsot", "ABCDEF_nebrelsot"]:
        count_tot = 0
        table_data = dict((x, {"BRENT_NFWI": {}, "AVG_NFWI": {}, "FTPAVG_NFWI": {}}) for x in parser_decays)
        for subdir, dirs, files in os.walk(rootd):
            for file in files:
                if 'thompson_newport_results.json' in file:
                    with open(subdir + "/thompson_newport_results.json", "r") as fpi:
                        data = json.load(fpi)
                    count_tot += 1
                    for pars_mem in parser_decays:
                        for met in ["BRENT_NFWI", "AVG_NFWI", "FTPAVG_NFWI"]:
                            for rip in [10, 100, 500, 1000, 5000, 10000]:
                                if rip not in table_data[pars_mem][met]:
                                    table_data[pars_mem][met][rip] = [0, 0]
                                gc = int(data[str(pars_mem)][str(rip)][met][fname]["g_hits"])
                                ggc = int(data[str(pars_mem)][str(rip)][met][fname]["gg_hits"])
                                table_data[pars_mem][met][rip][0] += gc
                                table_data[pars_mem][met][rip][1] += ggc

        table_data["total_count"] = count_tot
        table_data["count_mem"] = {}
        table_data["count_g_gg"] = {"G": 0, "GG_10": 0, "GG_100": 0, "GG_500": 0, "GG_1000": 0, "GG_5000": 0,
                                    "GG_10000": 0, }
        for pars_mem in parser_decays:
            table_data["count_mem"][pars_mem] = 0
            for met in ["BRENT_NFWI", "AVG_NFWI", "FTPAVG_NFWI"]:
                for rip in [10, 100, 500, 1000, 5000, 10000]:
                    table_data[pars_mem][met][rip][0] = round(table_data[pars_mem][met][rip][0] / count_tot)
                    table_data[pars_mem][met][rip][1] = round(table_data[pars_mem][met][rip][1] / count_tot)
                    if rip == 10:
                        table_data["count_mem"][pars_mem] += table_data[pars_mem][met][rip][0]
                        table_data["count_g_gg"]["G"] += table_data[pars_mem][met][rip][0]
                    table_data["count_mem"][pars_mem] += table_data[pars_mem][met][rip][1]
                    table_data["count_g_gg"]["GG_" + str(rip)] += table_data[pars_mem][met][rip][1]
        whole_d[fname] = table_data

    with open(rootd + "thompson_newport_table_data_nebrelsot.json", "w") as fpo:
        json.dump(whole_d, fpo)


def tables_data_thompson_newport_but_last(rootd):
    count_tot = 0
    table_data = dict((x, {"BRENT_NFWI": {}, "AVG_NFWI": {}, "FTPAVG_NFWI": {}}) for x in parser_decays)
    for subdir, dirs, files in os.walk(rootd):
        for file in files:
            if 'thompson_newport_results.json' in file:
                with open(subdir + "/thompson_newport_results.json", "r") as fpi:
                    data = json.load(fpi)
                count_tot += 1
                for pars_mem in parser_decays:
                    for met in ["BRENT_NFWI", "AVG_NFWI", "FTPAVG_NFWI"]:
                        for rip in [10, 100, 500, 1000, 5000, 10000]:
                            if rip not in table_data[pars_mem][met]:
                                table_data[pars_mem][met][rip] = [0, 0]
                            gc = int(data[str(pars_mem)][str(rip)][met]["but_last"]["g_hits"])
                            ggc = int(data[str(pars_mem)][str(rip)][met]["but_last"]["gg_hits"])
                            table_data[pars_mem][met][rip][0] += gc
                            table_data[pars_mem][met][rip][1] += ggc

    table_data["total_count"] = count_tot
    table_data["count_mem"] = {}
    table_data["count_g_gg"] = {"G": 0, "GG_10": 0, "GG_100": 0, "GG_500": 0, "GG_1000": 0, "GG_5000": 0, "GG_10000": 0}
    for pars_mem in parser_decays:
        table_data["count_mem"][pars_mem] = 0
        for met in ["BRENT_NFWI", "AVG_NFWI", "FTPAVG_NFWI"]:
            for rip in [10, 100, 500, 1000, 5000, 10000]:
                table_data[pars_mem][met][rip][0] = round(table_data[pars_mem][met][rip][0] / count_tot)
                table_data[pars_mem][met][rip][1] = round(table_data[pars_mem][met][rip][1] / count_tot)
                if rip == 10:
                    table_data["count_mem"][pars_mem] += table_data[pars_mem][met][rip][0]
                    table_data["count_g_gg"]["G"] += table_data[pars_mem][met][rip][0]
                table_data["count_mem"][pars_mem] += table_data[pars_mem][met][rip][1]
                table_data["count_g_gg"]["GG_" + str(rip)] += table_data[pars_mem][met][rip][1]

    with open(rootd + "thompson_newport_table_data_but_last.json", "w") as fpo:
        json.dump(table_data, fpo)


def tables_data_thompson_newport_train(rootd):
    count_tot = 0
    table_data = dict((x, {"BRENT_NFWI": {"G": [0, 0, 0, 0], "GG": {}}, "AVG_NFWI": {"G": [0, 0, 0, 0], "GG": {}},
                           "FTPAVG_NFWI": {"G": [0, 0, 0, 0], "GG": {}}}) for x in parser_decays)
    table_stats = dict(
        (x, {"BRENT_NFWI": {"G": [], "GG": {}}, "AVG_NFWI": {"G": [], "GG": {}}, "FTPAVG_NFWI": {"G": [], "GG": {}}})
        for x in parser_decays)
    for subdir, dirs, files in os.walk(rootd):
        for file in files:
            if 'thompson_newport_results.json' in file:
                with open(subdir + "/thompson_newport_results.json", "r") as fpi:
                    data = json.load(fpi)
                count_tot += 1
                for pars_mem in parser_decays:
                    for met in ["BRENT_NFWI", "AVG_NFWI", "FTPAVG_NFWI"]:
                        for rip in [10, 100, 500, 1000, 5000, 10000]:
                            if rip == 10:
                                table_data[pars_mem][met]["G"][0] += int(
                                    data[str(pars_mem)][str(rip)][met]["train"]["gen_hits"])
                                hrate = float(data[str(pars_mem)][str(rip)][met]["train"]["set_g"]) / int(
                                    data[str(pars_mem)][str(rip)][met]["train"]["gen_hits"])
                                table_data[pars_mem][met]["G"][1] += hrate
                                table_stats[pars_mem][met]["G"].append(
                                    int(data[str(pars_mem)][str(rip)][met]["train"]["gen_hits"]))

                            if rip not in table_data[pars_mem][met]["GG"]:
                                table_data[pars_mem][met]["GG"][rip] = [0, 0, 0, 0]
                                table_stats[pars_mem][met]["GG"][rip] = []

                            table_data[pars_mem][met]["GG"][rip][0] += int(
                                data[str(pars_mem)][str(rip)][met]["train"]["ggen_hits"])
                            gg_hrate = int(data[str(pars_mem)][str(rip)][met]["train"]["set_gg"]) / int(
                                data[str(pars_mem)][str(rip)][met]["train"]["ggen_hits"])
                            table_data[pars_mem][met]["GG"][rip][1] += gg_hrate
                            table_stats[pars_mem][met]["GG"][rip].append(
                                int(data[str(pars_mem)][str(rip)][met]["train"]["ggen_hits"]))

    for pars_mem in parser_decays:
        for met in ["BRENT_NFWI", "AVG_NFWI", "FTPAVG_NFWI"]:
            # G
            table_data[pars_mem][met]["G"][0] = round(table_data[pars_mem][met]["G"][0] / count_tot)
            table_data[pars_mem][met]["G"][1] = round(table_data[pars_mem][met]["G"][1] / count_tot, 2)
            table_data[pars_mem][met]["G"][2] = max(table_stats[pars_mem][met]["G"])
            table_data[pars_mem][met]["G"][3] = min(table_stats[pars_mem][met]["G"])

            for rip in [10, 100, 500, 1000, 5000, 10000]:
                # GG
                table_data[pars_mem][met]["GG"][rip][0] = round(table_data[pars_mem][met]["GG"][rip][0] / count_tot)
                table_data[pars_mem][met]["GG"][rip][1] = round(table_data[pars_mem][met]["GG"][rip][1] / count_tot, 2)
                table_data[pars_mem][met]["GG"][rip][2] = max(table_stats[pars_mem][met]["GG"][rip])
                table_data[pars_mem][met]["GG"][rip][3] = min(table_stats[pars_mem][met]["GG"][rip])

    table_data["total_count"] = count_tot
    with open(rootd + "thompson_newport_table_data_train.json", "w") as fpo:
        json.dump(table_data, fpo)
    with open(rootd + "thompson_newport_table_data_train_stdevs.json", "w") as fpo:
        json.dump(table_stats, fpo)


def draw_multibar_count_mem():
    # create data
    data = []

    df = pd.DataFrame([['Full', 397, 295, 232, 191, 163],
                       ['ABCDEF', 354, 312, 228, 181, 184]],
                      columns=['grammars', '20', '50', '100', '500', '1000'])
    # view data
    print(df)

    # plot grouped bar chart
    ax = df.plot(x='grammars',
                 kind='bar',
                 stacked=False,
                 # title='Total number of hits averaged by C (memory) values'
                 )
    ax.set(xlabel=None)
    # plt.show()
    plt.savefig('data/out/aggr_divergence1.pdf', bbox_inches='tight')


def draw_multibar_g_gg():
    # create data
    data = []

    df = pd.DataFrame([['Full', 93, 199, 204, 201, 196, 192, 193],
                       ['ABCDEF', 137, 166, 190, 190, 192, 188, 196]],
                      columns=['grammars', 'G', 'GG_10', 'GG_100', 'GG_500', 'GG_1000', 'GG_5000', 'GG_10000'])
    # view data
    print(df)

    # plot grouped bar chart
    ax = df.plot(x='grammars',
                 kind='bar',
                 stacked=False,
                 # title='Total number of hits aggregated by graph types'
                 )
    ax.set(xlabel=None)
    # plt.show()
    plt.legend(loc='upper center')
    plt.savefig('data/out/aggr_divergence1b.pdf', bbox_inches='tight')


def scatter_div2():
    with open("data/out/thompson_newport_table_data_train.json", "r") as fp:
        data = json.load(fp)

        table = {
            "G": {"H": [0, 0, 0], "HR": [0, 0, 0]},
            "GG_10": {"H": [0, 0, 0], "HR": [0, 0, 0]},
            "GG_100": {"H": [0, 0, 0], "HR": [0, 0, 0]},
            "GG_500": {"H": [0, 0, 0], "HR": [0, 0, 0]},
            "GG_1000": {"H": [0, 0, 0], "HR": [0, 0, 0]},
            "GG_5000": {"H": [0, 0, 0], "HR": [0, 0, 0]},
            "GG_10000": {"H": [0, 0, 0], "HR": [0, 0, 0]},
        }

        for c, c_vals in data.items():
            if c in ["20", "50", "100"]:
                for i, (met, met_vals) in enumerate(c_vals.items()):
                    print(i, " - ", met)
                    table["G"]["H"][i] += met_vals["G"][0]
                    table["G"]["HR"][i] += met_vals["G"][1]
                    for rip, r_vals in met_vals["GG"].items():
                        table["GG_" + str(rip)]["H"][i] += r_vals[0]
                        table["GG_" + str(rip)]["HR"][i] += r_vals[1]

    x = [_ / 3 for _ in table["G"]["H"]]
    y = [_ / 3 for _ in table["G"]["HR"]]

    plt.scatter(x, y, color=['red', 'green', 'blue'], alpha=0.5)
    ax = plt.gca()
    for i, txt in [(0, "BRENT"), (1, "AVG"), (2, "FTPAVG")]:
        ax.annotate(txt, (x[i], y[i]))
    # plt.ylim(min(y) - 0.02, max(y) + 0.02)
    plt.xlim(min(x) - 10, max(x) + 10)
    plt.xlabel("H")
    plt.ylabel("HR")
    # plt.title("Averaged results for methods: TPs graph")
    # plt.show()
    plt.savefig('data/out/aggr_div2_tps.pdf', bbox_inches='tight')

    for rip in ["10", "100", "500", "1000", "5000", "10000"]:
        plt.cla()
        plt.clf()
        x = [_ / 3 for _ in table["GG_" + rip]["H"]]
        y = [_ / 3 for _ in table["GG_" + rip]["HR"]]
        plt.scatter(x, y, color=['red', 'green', 'blue'], alpha=0.5)
        ax = plt.gca()
        for i, txt in [(0, "BRENT"), (1, "AVG"), (2, "FTPAVG")]:
            ax.annotate(txt, (x[i], y[i]))
        plt.xlim(min(x) - 10, max(x) + 10)
        plt.xlabel("H")
        plt.ylabel("HR")
        # plt.title("Averaged results for methods (repetitions = " + rip + "): generalized graph (GG)")
        # plt.show()
        plt.savefig('data/out/aggr_div2_gg' + rip + '.pdf', bbox_inches='tight')


def draw_stacked():
    plt.style.use('ggplot')

    df = pd.DataFrame({'a': [10, 20], 'b': [15, 25], 'c': [35, 40], 'd': [45, 50]}, index=['john', 'bob'])

    fig, ax = plt.subplots()
    df[['a', 'c']].plot.bar(stacked=True, width=0.1, position=1.5, colormap="bwr", ax=ax, alpha=0.7)
    df[['b', 'd']].plot.bar(stacked=True, width=0.1, position=-0.5, colormap="RdGy", ax=ax, alpha=0.7)
    df[['a', 'd']].plot.bar(stacked=True, width=0.1, position=0.5, colormap="BrBG", ax=ax, alpha=0.7)
    plt.legend(loc="upper center")
    plt.show()


def similarity_test():
    rep = ["mernebrelsotrudker",
           "mernebrelsotrudnav",
           "mernebrelsottafker"]

    a = "merlevrelsotrudsib"
    b = "mernebrelsotrudsib"
    c = "kofnebrelsottavluf"
    print(fuzz.token_sort_ratio(a, rep))
    print(fuzz.token_sort_ratio(c, rep))
    print(fuzz.partial_token_sort_ratio(a, "nebrelsot"))
    print(fuzz.partial_token_sort_ratio(c, "nebrelsot"))

    print((fuzz.token_sort_ratio(a, rep) + fuzz.partial_token_sort_ratio(a, "nebrelsot")) / 200)
    print((fuzz.token_sort_ratio(c, rep) + fuzz.partial_token_sort_ratio(c, "nebrelsot")) / 200)

    print((fuzz.token_sort_ratio(a, rep) + fuzz.partial_token_sort_ratio(a, rep)) / 200)
    print((fuzz.token_sort_ratio(c, rep) + fuzz.partial_token_sort_ratio(c, rep)) / 200)


def convergence_nebrelsot(rootd):
    tps_data = {}
    for pars_mem in [20]:
        tps_data[pars_mem] = {}
        rd = "tps_results_pars_{}/".format(pars_mem)
        for rip in [10, 100, 500]:
            tps_data[pars_mem][rip] = {}
            ripd = "tps_results_{}/".format(rip)
            for met in ["AVG_NFWI"]:
                tps_data[pars_mem][rip][met] = {}
                dirin = rootd + rd + ripd + "{}/".format(met)
                # convergence
                tps_data[pars_mem][rip][met]["ABCDEF_nebrelsot"] = {"u2": {}, "u3": {}}
                with open(dirin + "thompson_newport_ABCDEF_nebrelsot/gens_dataM2.json", "r") as fpi:
                    tps_data[pars_mem][rip][met]["ABCDEF_nebrelsot"]["u2"] = json.load(fpi)
                with open(dirin + "thompson_newport_ABCDEF_nebrelsot/gens_dataM3.json", "r") as fpi:
                    tps_data[pars_mem][rip][met]["ABCDEF_nebrelsot"]["u3"] = json.load(fpi)
    with open(rootd + "thompson_newport_results.json", "w") as fpo:
        json.dump(tps_data, fpo)


def tables_data_convergence_nebrelsot(rootd):
    rips = ["10","100","500"]
    pars_decs = ["20"]
    methods = ["AVG_NFWI"]
    for fname in ["ABCDEF_nebrelsot"]:
        count_tot = 0
        table_data = dict((x, {"AVG_NFWI": {"u2": {},"u3": {}}}) for x in pars_decs)
        for pars_mem in pars_decs:
            for met in methods:
                for un in ["u2", "u3"]:
                    table_data[pars_mem][met][un]["G"] = \
                        {"hits":0, "trends": {"mean": [0] * 1000, "max": [0] * 1000, "min": [0] * 1000}}
                    for rip in rips:
                        table_data[pars_mem][met][un]["GG_" + rip] = \
                            {"hits":0, "trends":{"mean": [0] * 1000, "max": [0] * 1000, "min": [0] * 1000}}
        for subdir, dirs, files in os.walk(rootd):
            if "cdr_" in subdir:
                for file in files:
                    if 'thompson_newport_results.json' in file:
                        with open(subdir + "/thompson_newport_results.json", "r") as fpi:
                            data = json.load(fpi)
                        print("processing: ", subdir)
                        count_tot += 1
                        # read data
                        for pars_mem in pars_decs:
                            for rip in rips:
                                for met in methods:
                                    for un in ["u2","u3"]:
                                        if rip == "10":
                                            # G
                                            table_data[pars_mem][met][un]["G"]["hits"] += int(data[pars_mem][rip][met][fname][un]["results"]["gen_hits"])
                                            table_data[pars_mem][met][un]["G"]["trends"]["mean"] = \
                                                [x + y for x, y in zip(table_data[pars_mem][met][un]["G"]["trends"]["mean"],
                                                                       data[pars_mem][rip][met][fname][un]["trends"]["G"]["mean"])]
                                            table_data[pars_mem][met][un]["G"]["trends"]["max"] = \
                                                [x + y for x, y in zip(table_data[pars_mem][met][un]["G"]["trends"]["max"],
                                                                       data[pars_mem][rip][met][fname][un]["trends"]["G"]["max"])]
                                            table_data[pars_mem][met][un]["G"]["trends"]["min"] = \
                                                [x + y for x, y in zip(table_data[pars_mem][met][un]["G"]["trends"]["min"],
                                                                       data[pars_mem][rip][met][fname][un]["trends"]["G"]["min"])]

                                        # GG
                                        table_data[pars_mem][met][un]["GG_"+rip]["hits"] += int(data[pars_mem][rip][met][fname][un]["results"]["ggen_hits"])
                                        table_data[pars_mem][met][un]["GG_"+rip]["trends"]["mean"] = \
                                            [x + y for x, y in zip(table_data[pars_mem][met][un]["GG_"+rip]["trends"]["mean"],
                                                                   data[pars_mem][rip][met][fname][un]["trends"]["GG"]["mean"])]
                                        table_data[pars_mem][met][un]["GG_"+rip]["trends"]["max"] = \
                                            [x + y for x, y in zip(table_data[pars_mem][met][un]["GG_"+rip]["trends"]["max"],
                                                                   data[pars_mem][rip][met][fname][un]["trends"]["GG"]["max"])]
                                        table_data[pars_mem][met][un]["GG_"+rip]["trends"]["min"] = \
                                            [x + y for x, y in zip(table_data[pars_mem][met][un]["GG_"+rip]["trends"]["min"],
                                                                   data[pars_mem][rip][met][fname][un]["trends"]["GG"]["min"])]

        # with open(rootd + "convergence_table_data_nebrelsot_intermed.json", "w") as fpo:
        #     json.dump(table_data, fpo)
        # table_data["total_count"] = count_tot
        count_us = {"u2":[],"u3":[]}
        for pars_mem in pars_decs:
            for met in methods:
                for un in ["u2","u3"]:
                    for k,v in table_data[pars_mem][met][un].items():
                        table_data[pars_mem][met][un][k]["hits"] = round(table_data[pars_mem][met][un][k]["hits"] / count_tot)
                        count_us[un].append(table_data[pars_mem][met][un][k]["hits"])
                        table_data[pars_mem][met][un][k]["trends"]["mean"] = [_ / count_tot for _ in table_data[pars_mem][met][un][k]["trends"]["mean"]]
                        table_data[pars_mem][met][un][k]["trends"]["max"] = [_ / count_tot for _ in table_data[pars_mem][met][un][k]["trends"]["max"]]
                        table_data[pars_mem][met][un][k]["trends"]["min"] = [_ / count_tot for _ in table_data[pars_mem][met][un][k]["trends"]["min"]]
                        plot_trends(table_data[pars_mem][met][un][k]["trends"], rootd, un + "_" + k)

    draw_multibar_convergence(count_us, rootd)
    with open(rootd + "convergence_table_data_nebrelsot.json", "w") as fpo:
        json.dump(table_data, fpo)


def draw_multibar_convergence(data, rootd):
    df = pd.DataFrame([["u2"]+data["u2"],
                       ["u3"]+data["u3"]],
                      columns=['utilities', 'G', 'GG_10', 'GG_100', 'GG_500'])
    # view data
    print(df)

    # plot grouped bar chart
    ax = df.plot(x='utilities',
                 kind='bar',
                 stacked=False,
                 # title='Total number of hits aggregated by graph types'
                 )
    ax.set(xlabel=None)
    # plt.show()
    plt.legend(loc='upper right')
    plt.savefig('data/out/aggr_utilities.pdf', bbox_inches='tight')


# read input model
def plot_trends(data,dir_out,sname):
    # Plotting the Data
    plt.cla()
    plt.clf()
    plt.plot(data["mean"], label='mean')
    plt.plot(data["max"], label='max')
    plt.plot(data["min"], label='min')
    plt.xlabel('iterations')
    plt.ylabel('C')
    # plt.title("Creativity values for " + sname)
    plt.legend()
    plt.savefig(dir_out + 'trends_' + sname + '.pdf')




if __name__ == "__main__":
    # rd = "data/CHILDES_converted/"
    # ro = "data/CHILDES_results2_BRENT_NFWI/"
    # os.makedirs(ro, exist_ok=True)
    # mdls = {
    #     "isp": "data/CHILDES_ISP/",
    #     "cbl": "data/CHILDES_cbl/",
    #     "tips": "data/CHILDES_tips2_BRENT_NFWI/"
    # }
    # collect_data(rd, ro, mdls)
    # collect_arrays(ro)
    # divergence_convergence_thompson_newport("data/out/convergence_divergence_results_4/")
    # divergence_convergence_thompson_newport("data/out/convergence_divergence_results_13/")
    # divergence_convergence_thompson_newport("data/out/convergence_divergence_results_77/")
    # divergence_convergence_thompson_newport("data/out/convergence_divergence_results_128/")
    # divergence_convergence_thompson_newport("data/out/convergence_divergence_results_142/")

    # convergence_nebrelsot("data/out/cdr_1_4/")
    # convergence_nebrelsot("data/out/cdr_1_13/")
    # convergence_nebrelsot("data/out/cdr_1_77/")
    # convergence_nebrelsot("data/out/cdr_1_128/")
    # convergence_nebrelsot("data/out/cdr_1_142/")
    tables_data_convergence_nebrelsot("data/out/")
    #
    # tables_data_thompson_newport_nebrelsot("data/out/")
    # tables_data_thompson_newport_train("data/out/")
    # tables_data_thompson_newport_but_last("data/out/")
    # draw_multibar_count_mem()
    # draw_multibar_g_gg()
    # scatter_div2()
    # draw_stacked()

    # similarity_test()
