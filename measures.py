"""Module for models tests and  measures"""

# from Learning a Generative Probabilistic Grammar of Experience: A Process-Level Model of Language Acquisition
# (https://onlinelibrary.wiley.com/doi/pdfdirect/10.1111/cogs.12140)
#  generative model can be evaluated with regard to its precision and
# recallâ€”two customary measures of performance in natural language engineering, which
# can address the perennial questions of model relevance and scalability.

# tests:
# both general (precision and recall)
# and specific (ranging from word segmentation to structure-dependent syntactic generalization).
# Characteristics of the learned representation: equivalence (substitutability)
# of phrases and the similarity structure of the phrase space
import math


def recall():
    """ defined as the proportion of sentences in a corpus withheld for testing that the model can generate
     (see Solan et al., 2005; for an earlier use and for a discussion of their roots in information retrieval)"""
    pass


def perplexity(model, sequences, n_words):
    """graded counterpart for recall:
     the (negative logarithm of the) mean probability assigned by the model to sentences from the test corpus
    (see, e.g., Goodman, 2001; for a definition)"""
    # They used a trained version of the model to calculate the production probability of each of the 100 utterances
    # in the test set, and the perplexity over it, using a standard formula (Jelinek, 1990; Stolcke, 2010):
    # where P(s) is the probability of a sentence s, the sum is over all the sentences in the test set,
    # and n is the number of words in the test set.
    tot = 0
    for s in sequences:
        tot += math.log(model.get_prob(s))
    return 10 ** - (tot/n_words)


def precision():
    """defined as the proportion of sentences generated by it that are found acceptable by human judges"""
    # To estimate the precision of the grammar learned by U-MILA and compare it to a trigram model,
    # they conducted two experiments in which participants were asked to rate the
    # acceptability of 50 sentences generated by each of the two models, which had been mixed with 50 sentences
    # from the original corpus (150 sentences altogether, ordered randomly).
    # Sentences were scored for their acceptability on a scale of 1 (not acceptable) to 7 (completely acceptable; Waterfall et al., 2010).
    # As the 50 sentences chosen from the original corpus ranged in length between three and eleven words,
    # in the analysis we excluded shorter and longer sentences generated...
    pass